# ===================================================================
# == Configuration for the Video Search Engine Ingestion Pipeline  ==
# ===================================================================

# --- General settings ---
general:
  # Device to use for ML models: "cuda", "cpu", "mps".
  # If "auto", the script will intelligently select CUDA if available, otherwise CPU.
  device: "auto"
  
  # Hugging Face access token for speaker diarization.
  hf_token: "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"

  # Default base directory for all processed video data.
  default_output_dir: "data/processed"

# --- UI Server Settings ---
ui:
  host: "127.0.0.1"
  port: 5050

# --- Default filenames for processed outputs ---
# These are relative to the specific video's output folder.
filenames:
  audio: "normalized_audio.mp3"
  raw_transcript: "transcript_raw.json"
  speaker_map: "speaker_map.json"
  transcript: "transcript_generic.json"
  shots: "shots.json"
  audio_events: "audio_events.json"
  visual_details: "visual_details.json"
  final_analysis: "final_analysis.json"
  final_segments: "final_segments.json" # Output from Step 2
  enriched_segments: "final_enriched_segments.json" # Output from Step 3

# --- LLM Enrichment Settings (Ollama) ---
llm_enrichment:
  enabled: true # Set to false to easily skip this step
  host: "http://localhost"
  port: 11434
  model: "gemma:7b" # The model to use (e.g., "gemma:2b", "llama3", etc.)
  timeout_sec: 120 # How long to wait for the LLM to respond

# --- Model identifiers and settings ---
models:
  # WhisperX for transcription
  transcription:
    name: "base"          # Model size: "tiny", "base", "small", "medium", "large-v2"
    compute_type: "int8"  # "float16", "int8", etc. depending on GPU support

  # Audio Spectrogram Transformer (AST) for event detection
  audio_events:
    name: "MIT/ast-finetuned-audioset-10-10-0.4593"

  # BLIP for image captioning
  visual_captioning:
    name: "Salesforce/blip-image-captioning-base"

# --- Parameters for processing steps ---
parameters:
  transcription:
    batch_size: 32

  audio:
    sample_rate: 16000 # Required sample rate for AST model

  audio_events:
    top_n: 3             # Number of top audio events to record per shot
    confidence_threshold: 0.1 # Minimum score for an event to be recorded

  visual_captioning:
    max_new_tokens: 50